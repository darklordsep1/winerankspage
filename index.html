<html>
<body style="background-color:rgb(216, 130, 130);">
<h1 >Wine Ranks</h1>
<img src="data:image/jpeg;base64,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">	
<h3>Nidhisha Dasari, Maansi Jain, Aanya Khandelwal, Sidharth Nair, Meghan Kulkarni</h3>

<h2>Introduction and Background</h2>
<p>Wine has been a popular drink for thousands of years and continues to be a highly sought after drink even today. There are many different factors for each wine besides the alcohol that determine how good it is, and by looking at these factors, wine quality can be better predicted. The dataset for this proposal is from Kaggle and includes a result of a red wine quality score based upon 11 other physicochemical properties . 

    In a study conducted by Gambetta, Cozzolino, Bastian, and Jeffrey (2016), they find a correlation between chardonnay juice and the compositions of wine from different regions of Europe. Additionally, in a study done by Sinton, Ough, Kissler, and Kasimatis (1978), there is correlation found between crop level and other factors such as intensity ratings. 
    </p>

<h2>Problem Definition</h2>
<p>This project is designed for people who are beginners in wine tasting. Wine can get expensive, and there is a lot of information that goes into picking a good quality. Beginners can get confused easily in choosing good quality wines versus bad quality ones. We hope to quell this confusion by having a place for beginners to choose a high rated wine based on certain factors like manufacturing, year, acidity, etc. Also, by having this project, beginners can save their money by not wasting it on bad wine. </p>

<h2>Data Collection</h2>
<a href= https://www.kaggle.com/datasets/rajyellow46/wine-quality>Wine Quality Dataset</a>
<p>This dataset comes from Kaggle. It combines two datasets that both relate to red and white variants of Portuguese “Vinho Verde” wine. It includes physicochemical and sensory variables. The classes of the data set are ordered and not balanced. There is no information listed about whether all the variables listed are applicable to determining the ranking of the wines.</p>

<h2>Methods</h2>
<h3>Preprocessed string values to numeric values in dataset</h3>
<p>Categorical data is easily understood by humans but machines cannot interpret and evaluate categorical data, so we have to convert the string data into numerical data. We wrote a method for preprocessing the string values in the dataframe into numerical values. First the categorical value ‘red’ was replaced with the numerical value ‘0’ and white was replaced with ‘1.’ For our analysis of the data, we have chosen to classify a wine as “good quality” if the given quality, between 0 and 10, is above 5. Therefore, the values for quality of wine were assigned based on their quality levels.</p>
<h3>Handled null values by replacing them with the mean</h3>
<p>To create a more efficient model, we chose to replace the null values in the dataset. Those null values can hinder the performance and cause noise when running the prediction model. We used Dataframe.fillna() from the pandas’ library to replace all these null values with the mean of that particular column from the dataframe. </p>
<h3>Normalized dataset</h3>
<p>Normalization of the dataset is necessary as it alters the numeric values in each column to fit a common scale. This allows for data to be observed and analyzed by accounting for the differences in ranges of values. Every dataset does not require normalization, but our dataset does due to it having different features which have varied ranges. In order to do this, we first converted the columns of our dataset to a list format and created a MinMaxScaler object which alters features by scaling each one to the same range. We used this in our dataset and then converted it to a table format using pd.DataFrame().</p>
<h3>Performed PCA with a retained variance of 97% on a normalized dataset</h3>
<p>PCA allows us to effectively preprocess our data components by cleaning the outside noise and creating principal components for better use for our supervised learning model. With our analysis, we chose 97% variance as it allows us to explain all of our principal components. After fitting and transforming the data, we return this revised dataframe. </p>
<h3>Performed random forest algorithm on the dataset</h3>
<p>The random forest algorithm is a supervised machine learning algorithm that uses the “bagging” method, or combining multiple learning models, in order to obtain a more accurate and stable prediction. We used this algorithm because it can be used for classification problems, which is how we are viewing our dataset. This method uses the random forest classifier class. Then, it splits the features and target into train and test sets and computes the accuracy score.</p>
<h3>Performed logistic regression on the dataset</h3>
<p>Logistic Regression is based on the logistic function which is also called the sigmoid function. It’s an S shaped curve that maps a real-valued number into a value between zero and one. The zero represents failure or no and the one represents success or yes. Essentially, logistic regression models the probability of the default class using an equation. Logistic regression is typically used in classification problems, which is fitting for our project, when the dependent variable is dichotomous or categorical. After running the logistic regression method on our dataset, we received an accuracy of 73.23%.</p>
<h3>Performed SGD on the dataset</h3>
<p>Stochastic gradient descent implies a process linked with random probability. Using the entire dataset is useful in order to get a minimal amount of noise and randomness. However, if the dataset is too big, it is too computationally expensive to complete all samples for each iteration. Instead of using the entire dataset, SGD uses only one sample which is randomly shuffled and selected to perform. Because Stochastic Gradient Descent is typically noisier than a typical Gradient descent, it takes more iterations to reach the minima which is okay because it is still computationally less expensive than GD. After running the SGD method on our dataset, we received an accuracy of 74.15%.</p>
<h3>Performed decision tree on the dataset</h3>
<p>Decision tree algorithm essentially functions to make a training model that can predict the value of a target variable, and it does this by learning or inferring decision rules from the set of training data. The structure is similar to a tree with nodes and branches in which it starts from the root. The internal nodes represent attributes while the leaf nodes are class labels. It gives predictions based upon the series of splits that occur due to features. Essentially, the sub-nodes created allow for more homogenous data as it splits based upon similarity as mentioned before. After running the decision tree method on our dataset, we received an accuracy of 77.38%.</p>

<h2>Results and Discussion</h2>
<p>Before we used any models, we first displayed our dataset to better illustrate the types of components for wine quality. </p>
<img src="images/BoxPlot.png" width="800" height="350">	
<p>We then preprocessed the data components by using PCA to clean the outside nose and create principal components to better use for our supervised learning model.  With our analysis, we chose 97% variance as it allows us to explain all of our principal components. After fitting and transforming the data, we ran all of the models. For our study, our team conducted 4 different model types to effectively measure wine quality. The 4 types of models we chose are the Random Forest, Logistic Regression, Stochastic Gradient, and Decision tree models. After running each model, we then ran a prediction function to record each model’s accuracy. </p>
<p>Here are the accuracy results from these modeling algorithms:<br>
    Accuracies: Random Forest Accuracy: 83.46%<br>
    Logistic Regression Accuracy: 73.23%<br>
    Stochastic Gradient Descent Accuracy: 74.15%<br>
    Decision Tree Accuracy: 77.38%<br>
    </p>
<h3>Created a confusion matrix</h3>
<p>Based on these accuracies, we can conclude that the Random Forest model performed the best.  To further analyze this data, we created a confusion matrix to plot any inaccuracies in the data predictions.  For those predictions that were incorrect, the confusion matrix displays where in fact the classifier made an incorrect prediction. We can observe in our created confusion matrix when the actual label and predicted label match in addition to when the classifier mislabeled observations.  The two rows represent the actual data predictions. The two columns represent the predicted data predictions. From the confusion matrix, we can analyze that 735 correct predictions were made with the actual performance. </p>
<img src="images/ConfusionMatrix.png">
<h3>Created a classification report</h3>
<p>We then created a classification report to measure the quality of the predictions from our algorithm. The prediction label checks to see what percentage of predictions are correct, recall checks what percent of the positive cases were caught by the algorithm, F1 checks what percent of positive predictions were correct, and support is the number of actual occurrences of the class in the specified dataset. These values are split between red and white wines, 0 for red and 1 for white. Looking at the results, we have a high precision value and a high recall value. A high precision (78% of red wine and 86% of white wine) based on the scale that more than half of our data components have true positives over false positive values. Based on the results, there was also an indication of a high recall percentage (75% of red wine and 88% of white wine) which indicates that our dataset has more true negative values versus false negatives. This creates a better overall model as more false positives and false negatives can negatively skew our prediction accuracy. This can also be indicated by our high  f1-score (.76 for red wine and .87 for white wine), which is an effective indicator of precision and recall. </p>
<img src="images/ClassificationReport.png">	

<h2>Conclusions</h2>
<p>From preprocessing the data components using PCA, we were able to receive accuracies of above 70% for each model. These results represent a strong indication that all of these algorithms can be used to predict wine quality. The success of these models can be attributed towards their strengths in handling large datasets with a lot of variability.</p>
<p>Based on our best model accuracy of 83% from the Random Forest Classifier, we can attribute that our model can effectively predict wine quality based on a variety of different factors such as acidity, density, sugar and sulfur dioxide. </p>
<p>In regards to the remaining gap of 17%, this can be attributed to the components being measured in the model. Since there was a set number of features included in the dataset, some of the other features that go into determining wine quality was omitted. Some of these features include opacity and viscosity. In addition to this, the dataset itself may not be representative of certain smells and tastes of wine that are deemed good quality by the general population. For future studies and research on the topic, it may be beneficial for these models to incorporate more features to effectively measure the quality. Although these models predict accuracy very well, our model does not predict the tastes, smells, and trends that society may have for certain factors of wine. For these measures, an incorporation into the dataset may be difficult but represents an important fact that wine quality is only as good as the wine drinker deems it to be. Altogether, our study concludes that the Random Forest Model most accurately predicted the key components to choose high quality wine.</p> 
    
<h2>References</h2>

<p>
    1. Bajaj, Aayush. “What Does Your Classification Metric Tell about Your Data?” Medium, Towards Data Science, 24 Mar. 2021, https://towardsdatascience.com/what-does-your-classification-metric-tell-about-your-data-4a8f35408a8b.</p>
<p>
    2. Brownlee, J. “Logistic regression for machine learning”. Machine Learning Mastery, April 27, 2022, https://machinelearningmastery.com/logistic-regression-for-machine-learning/.
</p>
<p>
    3. Donges, Niklas. “A Complete Guide to the Random Forest Algorithm.” Built In, 2021, https://builtin.com/data-science/random-forest-algorithm. 
</p>
<p>
    4. Gambetta, Joanna M., et al. “Towards the Creation of a Wine Quality Prediction Index: Correlation of Chardonnay Juice and Wine Compositions from Different Regions and Quality Levels - Food Analytical Methods.” SpringerLink, Springer US, 15 Mar. 2016, https://link.springer.com/article/10.1007/s12161-016-0467-9#Sec2. 
</p>
<p>
    5. Jaitley, Urvashi. “Why Data Normalization Is Necessary for Machine Learning Models.” Medium, Medium, 9 Apr. 2019, https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029. 
</p>
<p>
    6. Parmar, Raj. “Wine Quality.” Kaggle, 9 July 2018, https://www.kaggle.com/datasets/rajyellow46/wine-quality. 
</p>
<p>
    7. Shaw, Sweta. “Beginners Friendly - Comparison of Classifiers.” Kaggle, Kaggle, 4 Oct. 2021, https://www.kaggle.com/code/swetash/beginners-friendly-comparison-of-classifiers. 
</p>
<p>
    8. Shin, Terence. “Predicting Wine Quality with Several Classification Techniques.” Medium, Towards Data Science, 4 Dec. 2021, https://towardsdatascience.com/predicting-wine-quality-with-several-classification-techniques-179038ea6434. 
</p>
<p>
    9. Sinton, T. H., et al. “Grape Juice Indicators for Prediction of Potential Wine Quality. I. Relationship between Crop Level, Juice and Wine Composition, and Wine Sensory Ratings and Scores.” American Journal of Enology and Viticulture, American Journal of Enology and Viticulture, 1 Jan. 1978, https://www.ajevonline.org/content/29/4/267. 
</p>
<p>
    10. “Wine Quality Prediction Using Machine Learning: Predicting Wine Quality.” Analytics Vidhya, 27 Apr. 2021, https://www.analyticsvidhya.com/blog/2021/04/wine-quality-prediction-using-machine-learning/.
</p>
</body>
</html>
